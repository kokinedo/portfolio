---
title: "Building Production-Ready AI Applications: Architecture & Implementation"
summary: "Learn to build scalable AI applications with RAG, vector databases, API design, and production deployment strategies for real-world AI systems."
publishedAt: "2025-01-22"
tag: "AI Applications"
---

## The AI Application Stack

Building production-ready AI applications requires a sophisticated understanding of both AI/ML systems and traditional software architecture. The modern AI stack combines large language models, vector databases, robust APIs, and scalable infrastructure.

## RAG (Retrieval-Augmented Generation) Architecture

### Basic RAG Implementation

<CodeBlock
    marginBottom="16"
    codeInstances={[
      {
        code:
`from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
import os

class RAGSystem:
    def __init__(self, persist_directory="./chroma_db"):
        self.embeddings = OpenAIEmbeddings()
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.qa_chain = None
        
    def create_knowledge_base(self, documents):
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        texts = []
        metadatas = []
        
        for doc in documents:
            chunks = text_splitter.split_text(doc['content'])
            for chunk in chunks:
                texts.append(chunk)
                metadatas.append({
                    'source': doc['source'],
                    'title': doc.get('title', ''),
                    'chunk_id': len(texts)
                })
        
        self.vectorstore = Chroma.from_texts(
            texts=texts,
            metadatas=metadatas,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        self.vectorstore.persist()
        
        return len(texts)
    
    def load_knowledge_base(self):
        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings
        )
        
    def setup_qa_chain(self, llm_model="gpt-3.5-turbo"):
        llm = OpenAI(model_name=llm_model, temperature=0)
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 4}
            ),
            return_source_documents=True
        )
        
    def query(self, question):
        if not self.qa_chain:
            raise ValueError("QA chain not initialized. Call setup_qa_chain() first.")
            
        result = self.qa_chain({"query": question})
        
        return {
            "answer": result["result"],
            "sources": [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ]
        }`,
        language: "python",
        label: "Basic RAG System"
      }
    ]}
/>

### Advanced RAG with Reranking

<CodeBlock
    marginBottom="16"
    codeInstances={[
      {
        code:
`from sentence_transformers import CrossEncoder
from typing import List, Dict

class AdvancedRAGSystem(RAGSystem):
    def __init__(self, persist_directory="./chroma_db"):
        super().__init__(persist_directory)
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        
    def hybrid_search(self, query: str, top_k: int = 10, rerank_top_k: int = 4):
        initial_docs = self.vectorstore.similarity_search_with_score(
            query, k=top_k
        )
        
        doc_texts = [doc[0].page_content for doc in initial_docs]
        
        pairs = [[query, doc_text] for doc_text in doc_texts]
        rerank_scores = self.reranker.predict(pairs)
        
        doc_score_pairs = list(zip(initial_docs, rerank_scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        
        reranked_docs = [pair[0][0] for pair in doc_score_pairs[:rerank_top_k]]
        
        return reranked_docs
    
    def multi_query_retrieval(self, original_query: str):
        query_generation_prompt = f"""
You are an AI assistant that generates multiple search queries based on a single input query.
Generate 3 different but related search queries based on the following query:

Original query: {original_query}

Generated queries:
1.
2.
3.
"""
        
        generated_queries = self.generate_queries(query_generation_prompt)
        
        all_docs = []
        for query in [original_query] + generated_queries:
            docs = self.hybrid_search(query, top_k=6, rerank_top_k=2)
            all_docs.extend(docs)
        
        unique_docs = self.remove_duplicate_docs(all_docs)
        return unique_docs[:6]
    
    def contextual_compression(self, query: str, documents: List):
        """Compress retrieved documents to focus on relevant content"""
        
        compressed_docs = []
        
        for doc in documents:
            compression_prompt = f"""
Extract only the information from the following document that is directly relevant to this query: "{query}"

Document:
{doc.page_content}

Relevant extracted information:
"""
            
            compressed_content = self.llm.complete(compression_prompt)
            
            compressed_docs.append({
                "content": compressed_content,
                "metadata": doc.metadata,
                "original_length": len(doc.page_content),
                "compressed_length": len(compressed_content)
            })
            
        return compressed_docs`,
        language: "python",
        label: "Advanced RAG Features"
      }
    ]}
/>

## Vector Database Optimization

### Efficient Embedding Management

<CodeBlock
    marginBottom="16"
    codeInstances={[
      {
        code:
`import numpy as np
from typing import List, Tuple
import faiss
import pickle

class OptimizedVectorStore:
    def __init__(self, dimension: int = 1536, index_type: str = "IVF"):
        self.dimension = dimension
        self.index_type = index_type
        self.index = None
        self.document_store = {}
        self.id_counter = 0
        
    def create_index(self, num_clusters: int = None):
        """Create FAISS index for efficient similarity search"""
        
        if self.index_type == "IVF":
            # Index for large datasets with clustering
            if num_clusters is None:
                num_clusters = min(4096, max(4, self.id_counter // 1000))
                
            quantizer = faiss.IndexFlatL2(self.dimension)
            self.index = faiss.IndexIVFFlat(
                quantizer, self.dimension, num_clusters
            )
            
        elif self.index_type == "HNSW":
            # Hierarchical Navigable Small World index
            self.index = faiss.IndexHNSWFlat(self.dimension, 32)
            self.index.hnsw.efConstruction = 40
            self.index.hnsw.efSearch = 32
            
        else:
            # Simple flat index for small datasets
            self.index = faiss.IndexFlatL2(self.dimension)
    
    def add_embeddings(self, embeddings: np.ndarray, documents: List[Dict]):
        """Add embeddings and documents to the store"""
        
        if self.index is None:
            self.create_index()
            
        # Train index if needed (for IVF)
        if self.index_type == "IVF" and not self.index.is_trained:
            self.index.train(embeddings)
            
        # Add embeddings to index
        start_id = self.id_counter
        self.index.add_with_ids(
            embeddings.astype(np.float32),
            np.arange(start_id, start_id + len(embeddings))
        )
        
        # Store documents
        for i, doc in enumerate(documents):
            self.document_store[start_id + i] = doc
            
        self.id_counter += len(embeddings)
        
    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple]:
        """Search for similar documents"""
        
        query_embedding = query_embedding.astype(np.float32).reshape(1, -1)
        
        # Search index
        distances, indices = self.index.search(query_embedding, top_k)
        
        # Retrieve documents
        results = []
        for distance, idx in zip(distances[0], indices[0]):
            if idx != -1:  # Valid result
                doc = self.document_store.get(idx)
                if doc:
                    results.append((doc, float(distance)))
                    
        return results
    
    def save_index(self, filepath: str):
        """Save index and document store"""
        # Save FAISS index
        faiss.write_index(self.index, f"{filepath}.faiss")
        
        # Save document store
        with open(f"{filepath}.docs", "wb") as f:
            pickle.dump(self.document_store, f)
            
        # Save metadata
        metadata = {
            "dimension": self.dimension,
            "index_type": self.index_type,
            "id_counter": self.id_counter
        }
        with open(f"{filepath}.meta", "wb") as f:
            pickle.dump(metadata, f)
    
    def load_index(self, filepath: str):
        """Load index and document store"""
        # Load FAISS index
        self.index = faiss.read_index(f"{filepath}.faiss")
        
        # Load document store
        with open(f"{filepath}.docs", "rb") as f:
            self.document_store = pickle.load(f)
            
        # Load metadata
        with open(f"{filepath}.meta", "rb") as f:
            metadata = pickle.load(f)
            self.dimension = metadata["dimension"]
            self.index_type = metadata["index_type"]
            self.id_counter = metadata["id_counter"]`,
        language: "python",
        label: "Optimized Vector Store"
      }
    ]}
/>

## API Design for AI Applications

### RESTful AI Service API

<CodeBlock
    marginBottom="16"
    codeInstances={[
      {
        code:
`from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import asyncio
import uuid
from datetime import datetime

app = FastAPI(
    title="AI Assistant API",
    description="Production-ready AI application API",
    version="1.0.0"
)

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security
security = HTTPBearer()

class ChatMessage(BaseModel):
    role: str = Field(..., regex="^(user|assistant|system)$")
    content: str
    timestamp: Optional[datetime] = None

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    model: str = "gpt-3.5-turbo"
    temperature: float = Field(0.7, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(None, gt=0, le=4096)
    stream: bool = False
    
class ChatResponse(BaseModel):
    id: str
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]
    created: int

class DocumentRequest(BaseModel):
    content: str
    title: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    
class SearchRequest(BaseModel):
    query: str
    top_k: int = Field(5, ge=1, le=20)
    filters: Optional[Dict[str, Any]] = None

class SearchResult(BaseModel):
    content: str
    metadata: Dict[str, Any]
    score: float

class SearchResponse(BaseModel):
    results: List[SearchResult]
    total_found: int
    query_time_ms: float

# Dependency for API key validation
async def verify_api_key(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Verify API key"""
    # Implement your API key validation logic
    if not validate_api_key(credentials.credentials):
        raise HTTPException(status_code=401, detail="Invalid API key")
    return credentials.credentials

@app.post("/v1/chat/completions", response_model=ChatResponse)
async def create_chat_completion(
    request: ChatRequest,
    api_key: str = Depends(verify_api_key)
):
    """Create a chat completion"""
    try:
        # Process chat request
        response_id = str(uuid.uuid4())
        
        # Call your AI model
        response_content = await process_chat_request(request)
        
        # Format response
        response = ChatResponse(
            id=response_id,
            model=request.model,
            choices=[{
                "message": {
                    "role": "assistant",
                    "content": response_content
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": count_tokens(request.messages),
                "completion_tokens": count_tokens([response_content]),
                "total_tokens": 0  # Calculate actual total
            },
            created=int(datetime.now().timestamp())
        )
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/documents", status_code=201)
async def add_document(
    request: DocumentRequest,
    background_tasks: BackgroundTasks,
    api_key: str = Depends(verify_api_key)
):
    """Add document to knowledge base"""
    try:
        doc_id = str(uuid.uuid4())
        
        # Add document processing to background tasks
        background_tasks.add_task(
            process_document,
            doc_id,
            request.content,
            request.title,
            request.metadata
        )
        
        return {"document_id": doc_id, "status": "processing"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/search", response_model=SearchResponse)
async def search_documents(
    request: SearchRequest,
    api_key: str = Depends(verify_api_key)
):
    """Search documents in knowledge base"""
    try:
        start_time = datetime.now()
        
        # Perform search
        results = await perform_search(
            request.query,
            request.top_k,
            request.filters
        )
        
        end_time = datetime.now()
        query_time = (end_time - start_time).total_seconds() * 1000
        
        # Format results
        search_results = [
            SearchResult(
                content=result["content"],
                metadata=result["metadata"],
                score=result["score"]
            )
            for result in results
        ]
        
        return SearchResponse(
            results=search_results,
            total_found=len(results),
            query_time_ms=query_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/v1/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }`,
        language: "python",
        label: "AI Service API"
      }
    ]}
/>

## Streaming and Real-time Features

### WebSocket for Real-time AI Responses

<CodeBlock
    marginBottom="16"
    codeInstances={[
      {
        code:
`from fastapi import WebSocket, WebSocketDisconnect
import json
import asyncio

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.user_sessions: Dict[str, Dict] = {}
        
    async def connect(self, websocket: WebSocket, user_id: str):
        await websocket.accept()
        self.active_connections.append(websocket)
        self.user_sessions[user_id] = {
            "websocket": websocket,
            "conversation_history": [],
            "connected_at": datetime.now()
        }
        
    def disconnect(self, websocket: WebSocket, user_id: str):
        self.active_connections.remove(websocket)
        if user_id in self.user_sessions:
            del self.user_sessions[user_id]
            
    async def send_personal_message(self, message: str, user_id: str):
        if user_id in self.user_sessions:
            websocket = self.user_sessions[user_id]["websocket"]
            await websocket.send_text(message)
            
    async def stream_ai_response(self, user_id: str, prompt: str):
        """Stream AI response token by token"""
        if user_id not in self.user_sessions:
            return
            
        websocket = self.user_sessions[user_id]["websocket"]
        
        try:
            # Start response
            await websocket.send_text(json.dumps({
                "type": "response_start",
                "message_id": str(uuid.uuid4())
            }))
            
            # Stream tokens
            async for token in stream_llm_response(prompt):
                await websocket.send_text(json.dumps({
                    "type": "token",
                    "content": token
                }))
                
                # Small delay to prevent overwhelming
                await asyncio.sleep(0.01)
                
            # End response
            await websocket.send_text(json.dumps({
                "type": "response_end"
            }))
            
        except Exception as e:
            await websocket.send_text(json.dumps({
                "type": "error",
                "message": str(e)
            }))

manager = ConnectionManager()

@app.websocket("/ws/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    await manager.connect(websocket, user_id)
    
    try:
        while True:
            # Receive message from client
            data = await websocket.receive_text()
            message_data = json.loads(data)
            
            if message_data["type"] == "chat_message":
                # Add to conversation history
                manager.user_sessions[user_id]["conversation_history"].append(
                    message_data["content"]
                )
                
                # Stream AI response
                await manager.stream_ai_response(
                    user_id, 
                    message_data["content"]
                )
                
            elif message_data["type"] == "search_query":
                # Handle search request
                results = await perform_search(message_data["query"])
                await websocket.send_text(json.dumps({
                    "type": "search_results",
                    "results": results
                }))
                
    except WebSocketDisconnect:
        manager.disconnect(websocket, user_id)`,
        language: "python",
        label: "WebSocket Streaming"
      }
    ]}
/>

## Production Deployment and Scaling

### Containerized Deployment

<CodeBlock
    marginBottom="16"
    codeInstances={[
      {
        code:
`# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    build-essential \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN useradd --create-home --shell /bin/bash app
RUN chown -R app:app /app
USER app

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\
    CMD curl -f http://localhost:8000/v1/health || exit 1

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]`,
        language: "docker",
        label: "Dockerfile"
      }
    ]}
/>

<CodeBlock
    marginBottom="16"
    codeInstances={[
      {
        code:
`# docker-compose.yml
version: '3.8'

services:
  ai-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=\${OPENAI_API_KEY}
      - DATABASE_URL=postgresql://user:password@postgres:5432/aidb
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis
      - vector-db
    volumes:
      - ./models:/app/models
    restart: unless-stopped
    
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=aidb
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
      
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      
  vector-db:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
      
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - ai-app

volumes:
  postgres_data:
  redis_data:
  qdrant_data:`,
        language: "yaml",
        label: "Docker Compose"
      }
    ]}
/>

### Kubernetes Deployment

<CodeBlock
    marginBottom="16"
    codeInstances={[
      {
        code:
`# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-app
  labels:
    app: ai-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-app
  template:
    metadata:
      labels:
        app: ai-app
    spec:
      containers:
      - name: ai-app
        image: your-registry/ai-app:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-secrets
              key: openai-api-key
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: ai-secrets
              key: database-url
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /v1/health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /v1/health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: ai-app-service
spec:
  selector:
    app: ai-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80`,
        language: "yaml",
        label: "Kubernetes Deployment"
      }
    ]}
/>

Build production-ready AI applications that scale. Remember: the key to successful AI applications is not just the AI model, but the entire system architecture that supports it reliably at scale.